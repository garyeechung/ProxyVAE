{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d7fc51",
   "metadata": {},
   "source": [
    "# MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56798c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set the GPUs to use\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from src.mnist.datasets import get_mnist_dataloaders, convert_flattened_to_image, get_merged_labels\n",
    "from src.mnist.models import CVAE, InvariantAutoEncoder, InvariantVariationalAutoEncoder, ProxyRep2InvarRep, ProxyRep2Label\n",
    "from src.mnist.losses import CVAE_Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a7551",
   "metadata": {},
   "source": [
    "# Merged group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "BETA = 5e-4  # Scaled by batch size for stability\n",
    "REDUCTION = 'mean'\n",
    "EPOCHS = 50\n",
    "LR = 5e-4  # Learning rate\n",
    "CVAE_PATH = \"../checkpoints/mnist/group_similar/cvae.pth\"\n",
    "\n",
    "mnist_train, mnist_val, mnist_test = get_mnist_dataloaders(\"../data\", one_hot=False)\n",
    "\n",
    "merge_group = [\n",
    "    [0, 6],\n",
    "    [1],\n",
    "    [4, 7, 9],\n",
    "    [2, 3, 5, 8]\n",
    "]\n",
    "\n",
    "cvae = CVAE(28 * 28, 128, 64, num_classes=4)\n",
    "cvae = cvae.to(\"cuda\")\n",
    "\n",
    "if os.path.exists(CVAE_PATH):\n",
    "    print(\"Loading pre-trained CVAE model...\")\n",
    "    cvae.load_state_dict(torch.load(CVAE_PATH, map_location=device))\n",
    "else:\n",
    "    print(\"No pre-trained model found. Starting from scratch.\")\n",
    "    optimizer = optim.Adam(cvae.parameters(), lr=LR)\n",
    "    loss_fn = CVAE_Loss(beta=BETA, reduction=REDUCTION)\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        cvae.train()\n",
    "        total_loss = 0\n",
    "        for i, (x, y) in enumerate(mnist_train):\n",
    "            x = x.to(device)\n",
    "            y = get_merged_labels(y, merge_group).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_x, mu, logvar = cvae(x, y)\n",
    "            loss = loss_fn(recon_x, x, mu, logvar)\n",
    "            loss.backward()\n",
    "            losses_train.append(loss.item())\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            print(f\"{i + 1:>3}/{len(mnist_train)}: {loss.item():.4f}\", end=\"\\r\")\n",
    "        with torch.no_grad():\n",
    "            cvae.eval()\n",
    "            total_val_loss = 0\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                y = get_merged_labels(y, merge_group).to(device)\n",
    "                recon_x, mu, logvar = cvae(x, y)\n",
    "                val_loss = loss_fn(recon_x, x, mu, logvar)\n",
    "                total_val_loss += val_loss.item()\n",
    "            avg_val_loss = total_val_loss / len(mnist_val)\n",
    "            losses_val.append(avg_val_loss)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    torch.save(cvae.state_dict(), CVAE_PATH)\n",
    "\n",
    "    nb_minibatches = len(mnist_train)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses_train, label='Training Loss', color='blue')\n",
    "    plt.plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "             losses_val, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Batch Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_SAMPLES = 8\n",
    "\n",
    "# MERGE_GROUP = [\n",
    "#     [0, 6],\n",
    "#     [1],\n",
    "#     [4, 7, 9],\n",
    "#     [2, 3, 5, 8]\n",
    "# ]\n",
    "\n",
    "for x, y_real in mnist_test:\n",
    "    # print(x.shape, y.shape)\n",
    "    x = x.to(device)\n",
    "    y = get_merged_labels(y_real)\n",
    "    y = y.to(device)\n",
    "    break\n",
    "\n",
    "cvae.encoder(x)\n",
    "y_shuffled = y[torch.randperm(y.size(0))]\n",
    "x_shuffled_recon, mu, logvar = cvae(x, y_shuffled)\n",
    "x_recon = cvae(x, y)[0]\n",
    "\n",
    "fig, axs = plt.subplots(3, NB_SAMPLES, figsize=(NB_SAMPLES*2, 6))\n",
    "for idx in range(NB_SAMPLES):\n",
    "    axs[0, idx].imshow(x[idx].cpu().reshape(28, 28), cmap=\"gray\")\n",
    "    axs[0, idx].set_title(f\"{y_real[idx].item()} in {merge_group[y[idx].argmax()]}\")\n",
    "    axs[0, idx].set_xticks([])\n",
    "    axs[0, idx].set_yticks([])\n",
    "\n",
    "    axs[1, idx].imshow(x_recon[idx].detach().cpu().reshape(28, 28), cmap=\"gray\")\n",
    "    axs[1, idx].set_title(f\" \")\n",
    "    axs[1, idx].set_xticks([])\n",
    "    axs[1, idx].set_yticks([])\n",
    "\n",
    "    axs[2, idx].imshow(x_shuffled_recon[idx].detach().cpu().reshape(28, 28), cmap=\"gray\")\n",
    "    axs[2, idx].set_title(f\"$S$ = {merge_group[y_shuffled[idx].argmax()]}\")\n",
    "    axs[2, idx].set_xticks([])\n",
    "    axs[2, idx].set_yticks([])\n",
    "axs[0, 0].set_ylabel(\"Original\")\n",
    "axs[1, 0].set_ylabel(\"Reconstructed\")\n",
    "axs[2, 0].set_ylabel(\"Shuffled\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b8967",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2_DIM = 4  # Dimension of the latent space for invariant VAE\n",
    "INVAE_PATH = \"../checkpoints/mnist/group_similar/invariant_vae.pth\"\n",
    "# make cvae not trainable\n",
    "for param in cvae.parameters():\n",
    "    param.requires_grad = False\n",
    "# make invariant vae trainable\n",
    "invae = InvariantVariationalAutoEncoder(28 * 28, 128, Z2_DIM, cvae=cvae)\n",
    "invae = invae.to(device)\n",
    "\n",
    "if os.path.exists(INVAE_PATH):\n",
    "    print(\"Loading pre-trained Invariant AE model...\")\n",
    "    invae.load_state_dict(torch.load(INVAE_PATH, map_location=device))\n",
    "else:\n",
    "    print(\"No pre-trained model found. Starting from scratch.\")\n",
    "\n",
    "    optimizer = optim.Adam(invae.parameters(), lr=1e-3)\n",
    "    loss_fn = CVAE_Loss(beta=BETA, reduction=REDUCTION)\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        invae.train()\n",
    "        for i, (x, y) in enumerate(tqdm(mnist_train)):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_x, mu, logvar = invae(x)\n",
    "            loss = loss_fn(recon_x, x, mu, logvar)\n",
    "            loss.backward()\n",
    "            losses_train.append(loss.item())\n",
    "            optimizer.step()\n",
    "            # print(f\"{i + 1:>3}/{len(mnist_train)}: {loss.item():.4f}\", end=\"\\r\")\n",
    "        with torch.no_grad():\n",
    "            invae.eval()\n",
    "            total_val_loss = 0\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                recon_x, mu, logvar = invae(x)\n",
    "                val_loss = loss_fn(recon_x, x, mu, logvar)\n",
    "                total_val_loss += val_loss.item()\n",
    "            avg_val_loss = total_val_loss / len(mnist_val)\n",
    "            losses_val.append(avg_val_loss)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "    torch.save(invae.state_dict(), INVAE_PATH)\n",
    "\n",
    "    nb_minibatches = len(mnist_train)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses_train, label='Training Loss', color='blue')\n",
    "    plt.plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "             losses_val, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Batch Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_SAMPLES = 8\n",
    "NB_subplots = 8\n",
    "nb_figures = NB_SAMPLES // NB_subplots\n",
    "\n",
    "for x, y_real in mnist_test:\n",
    "    # print(x.shape, y.shape)\n",
    "    x = x.to(device)\n",
    "    y = get_merged_labels(y_real)\n",
    "    y = y.to(device)\n",
    "    break\n",
    "\n",
    "x_recon_cvae = cvae(x, y)[0]\n",
    "y_shuffled = y[torch.randperm(y.size(0))]\n",
    "x_shuffled_recon, mu, logvar = cvae(x, y_shuffled)\n",
    "x_recon_invae = invae(x)[0]\n",
    "\n",
    "for f in range(nb_figures):\n",
    "    fig, axs = plt.subplots(4, NB_subplots, figsize=(NB_subplots*2, 8), tight_layout=True)\n",
    "    for idx in range(min(NB_subplots, NB_SAMPLES - f * NB_subplots)):\n",
    "        axs[0, idx].imshow(x[f * NB_subplots + idx].cpu().reshape(28, 28), cmap=\"gray\")\n",
    "        axs[0, idx].set_title(f\"{y_real[idx].item()} in {merge_group[y[idx].argmax()]}\")\n",
    "        axs[0, idx].set_xticks([])\n",
    "        axs[0, idx].set_yticks([])\n",
    "\n",
    "        axs[1, idx].imshow(x_recon_cvae[f * NB_subplots + idx].detach().cpu().reshape(28, 28), cmap=\"gray\")\n",
    "        axs[1, idx].set_title(\" \")\n",
    "        axs[1, idx].set_xticks([])\n",
    "        axs[1, idx].set_yticks([])\n",
    "\n",
    "        axs[2, idx].imshow(x_shuffled_recon[idx].detach().cpu().reshape(28, 28), cmap=\"gray\")\n",
    "        axs[2, idx].set_title(f\"$S$ = {merge_group[y_shuffled[idx].argmax()]}\")\n",
    "        axs[2, idx].set_xticks([])\n",
    "        axs[2, idx].set_yticks([])\n",
    "\n",
    "        axs[3, idx].imshow(x_recon_invae[f * NB_subplots + idx].detach().cpu().reshape(28, 28), cmap=\"gray\")\n",
    "        axs[3, idx].set_title(\" \")\n",
    "        axs[3, idx].set_xticks([])\n",
    "        axs[3, idx].set_yticks([])\n",
    "    axs[0, 0].set_ylabel(\"Original\", fontsize=15)\n",
    "    axs[1, 0].set_ylabel(\"CVAE Recon.\", fontsize=15)\n",
    "    axs[2, 0].set_ylabel(\"Shuffled Recon.\", fontsize=15)\n",
    "    axs[3, 0].set_ylabel(\"InvVAE Recon.\", fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ef114",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 3\n",
    "x = []\n",
    "y = []\n",
    "for x_tmp, y_tmp in mnist_test:\n",
    "    x.append(x_tmp)\n",
    "    y.append(y_tmp)\n",
    "x = torch.cat(x).to(device)\n",
    "y = torch.cat(y).to(device)\n",
    "label = y.detach().cpu().flatten()\n",
    "x_recon_inv_vae = invae(x)\n",
    "z = invae.encoder(x)\n",
    "mu = z[:, :invae.latent_dim]\n",
    "logvar = z[:, invae.latent_dim:]\n",
    "z = invae.reparameterize(mu, logvar)\n",
    "z = z.detach().cpu()\n",
    "z_centers = []\n",
    "for i in range(10):\n",
    "    center = z[label == i].mean(axis=0)\n",
    "    z_centers.append(center)\n",
    "z_centers = torch.stack(z_centers)\n",
    "\n",
    "fig, axes = plt.subplots(Z2_DIM - 1, Z2_DIM - 1, figsize=(5 * (Z2_DIM - 1), 5 * (Z2_DIM - 1)), tight_layout=True)\n",
    "for i in range(Z2_DIM - 1):\n",
    "    for j in range(1, Z2_DIM):\n",
    "        if i < j:\n",
    "            im = axes[i, j-1].scatter(z_centers[:, i], z_centers[:, j],\n",
    "                                    c=range(10), cmap=\"tab10\", vmin=-.5, vmax=9.5)\n",
    "            axes[i, j-1].scatter(z[:, i], z[:, j],\n",
    "                            c=label, cmap=\"tab10\", alpha=0.1)\n",
    "            # axes[i, j-1].set_aspect('equal', adjustable='box')\n",
    "            # axes[i, j-1].set_xlim(-LIMIT, LIMIT)\n",
    "            # axes[i, j-1].set_ylim(-LIMIT, LIMIT)\n",
    "            for d in range(10):\n",
    "                axes[i, j-1].text(z_centers[d][i], z_centers[d][j], str(d), fontsize=15, ha='center', va='center')\n",
    "        else:\n",
    "            axes[i, j-1].axis('off')\n",
    "\n",
    "for i in range(Z2_DIM - 1):\n",
    "    for j in range(Z2_DIM - 1):\n",
    "        if i == j :\n",
    "            axes[i, j].set_ylabel(f\"$z_2$[{i}]\", fontsize=15)\n",
    "            axes[i, j].set_xlabel(f\"$z_2$[{j + 1}]\", fontsize=15)\n",
    "        elif i > j:\n",
    "            axes[i, j].axis('off')\n",
    "        # else:\n",
    "        #     axes[i, j].set_xticks([])\n",
    "        #     axes[i, j].set_yticks([])\n",
    "\n",
    "plt.suptitle(\"Latent Space of Invariant VAE\", fontsize=20)\n",
    "cbar = plt.colorbar(im, ax=axes, orientation='horizontal',\n",
    "                    fraction=0.02, pad=0.0)\n",
    "cbar.set_label('Digit Label', fontsize=15)\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.set_ticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5998842",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY2INVAR_PATH = \"../checkpoints/mnist/group_similar/vaeproxy2invar.pth\"\n",
    "\n",
    "for param in invae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "proxy2invar = ProxyRep2InvarRep(autoencoder=invae, reparameterize=True)\n",
    "proxy2invar = proxy2invar.to(device)\n",
    "\n",
    "if os.path.exists(PROXY2INVAR_PATH):\n",
    "    print(\"Loading pre-trained Proxy2Invar model...\")\n",
    "    proxy2invar.load_state_dict(torch.load(PROXY2INVAR_PATH, map_location=device))\n",
    "else:\n",
    "    print(\"No pre-trained model found. Starting from scratch.\")\n",
    "    optimizer = optim.Adam(proxy2invar.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        proxy2invar.train()\n",
    "        for i, (x, y) in enumerate(tqdm(mnist_train)):\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # Get real invariant representation from CVAE\n",
    "            with torch.no_grad():\n",
    "                z_invar = invae.cvae.encoder(x)\n",
    "                mu = z_invar[:, :invae.cvae.latent_dim]\n",
    "                logvar = z_invar[:, invae.cvae.latent_dim:]\n",
    "                z_invar = invae.cvae.reparameterize(mu, logvar)\n",
    "\n",
    "            z_invar_pred = proxy2invar(x)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(z_invar_pred, z_invar)\n",
    "            loss.backward()\n",
    "            losses_train.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "        proxy2invar.eval()\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                z_invar = invae.cvae.encoder(x)\n",
    "                mu = z_invar[:, :invae.cvae.latent_dim]\n",
    "                logvar = z_invar[:, invae.cvae.latent_dim:]\n",
    "                z_invar = invae.cvae.reparameterize(mu, logvar)\n",
    "\n",
    "                z_invar_pred = proxy2invar(x)\n",
    "                val_loss = loss_fn(z_invar_pred, z_invar)\n",
    "                total_val_loss += val_loss.item()\n",
    "            avg_val_loss = total_val_loss / len(mnist_val)\n",
    "            losses_val.append(avg_val_loss)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "    torch.save(proxy2invar.state_dict(), PROXY2INVAR_PATH)\n",
    "\n",
    "    nb_minibatches = len(mnist_train)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses_train, label='Training Loss', color='blue')\n",
    "    plt.plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                losses_val, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Batch Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('VAE Proxy to Invariant Representation MSE')\n",
    "    plt.legend()\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(0.94, 1.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy2group = ProxyRep2Label(autoencoder=invae, reparameterize=True, nb_labels=len(merge_group))\n",
    "proxy2group.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY2GROUP_PATH = \"../checkpoints/mnist/group_similar/vaeproxy2group.pth\"\n",
    "\n",
    "proxy2group = ProxyRep2Label(autoencoder=invae, reparameterize=True, nb_labels=len(merge_group))\n",
    "proxy2group = proxy2group.to(device)\n",
    "if os.path.exists(PROXY2GROUP_PATH):\n",
    "    print(\"Loading pre-trained Proxy2Group model...\")\n",
    "    proxy2group.load_state_dict(torch.load(PROXY2GROUP_PATH, map_location=device))\n",
    "else:\n",
    "    print(\"No pre-trained model found. Starting from scratch.\")\n",
    "    optimizer = optim.Adam(proxy2group.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracies_val = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        proxy2group.train()\n",
    "        for i, (x, y) in enumerate(tqdm(mnist_train)):\n",
    "            x = x.to(device)\n",
    "            y = get_merged_labels(y, merge_group).float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = proxy2group(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            losses_train.append(loss.item())\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            proxy2group.eval()\n",
    "            total_val_loss = 0\n",
    "            total_val_accu = 0\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                y = get_merged_labels(y, merge_group).float().to(device)\n",
    "                y_pred = proxy2group(x)\n",
    "                val_loss = loss_fn(y_pred, y)\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "                group_pred = y_pred.argmax(dim=1)\n",
    "                group_true = y.argmax(dim=1)\n",
    "                acc = (group_pred == group_true).float().mean().item()\n",
    "                total_val_accu += acc\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(mnist_val)\n",
    "            avg_val_accu = total_val_accu / len(mnist_val)\n",
    "            accuracies_val.append(avg_val_accu)\n",
    "            losses_val.append(avg_val_loss)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}, Accuracy: {avg_val_accu:.4f}\")\n",
    "    torch.save(proxy2group.state_dict(), PROXY2GROUP_PATH)\n",
    "\n",
    "    nb_minibatches = len(mnist_train)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    axes[0].plot(losses_train, label='Training Loss', color='blue')\n",
    "    axes[0].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                losses_val, label='Validation Loss', color='orange')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('VAE Proxy to Label Cross Entropy')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_xticks([])\n",
    "    xlim = axes[0].get_xlim()\n",
    "\n",
    "    axes[1].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                accuracies_val, label='Validation Accuracy', color='green')\n",
    "    axes[1].set_xlabel('Batch Iterations')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('VAE Proxy to Label Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].set_xlim(xlim)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy2group.eval()\n",
    "\n",
    "group_true_all = []\n",
    "group_pred_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in mnist_test:\n",
    "        x = x.to(device)\n",
    "        y = get_merged_labels(y, merge_group).float().to(device)\n",
    "        y_pred = proxy2group(x)\n",
    "        group_pred = y_pred.argmax(dim=1)\n",
    "        group_true = y.argmax(dim=1)\n",
    "\n",
    "        group_true_all.append(group_true.cpu())\n",
    "        group_pred_all.append(group_pred.cpu())\n",
    "group_true_all = torch.cat(group_true_all)\n",
    "group_pred_all = torch.cat(group_pred_all)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(group_true_all, group_pred_all)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.xlabel('Predicted Group')\n",
    "plt.ylabel('True Group')\n",
    "plt.title('Confusion Matrix for Group Predictions')\n",
    "plt.xticks(range(len(merge_group)), [str(g) for g in merge_group], rotation=45)\n",
    "plt.yticks(range(len(merge_group)), [str(g) for g in merge_group])\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036744c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY2LAB_PATH = \"../checkpoints/mnist/group_similar/vaeproxy2lab.pth\"\n",
    "\n",
    "proxy2lab = ProxyRep2Label(autoencoder=invae, reparameterize=True, nb_labels=10)\n",
    "proxy2lab = proxy2lab.to(device)\n",
    "mnist_train, mnist_val, mnist_test = get_mnist_dataloaders(\"../data\", one_hot=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "if os.path.exists(PROXY2LAB_PATH):\n",
    "    print(\"Loading pre-trained Proxy2Label model...\")\n",
    "    proxy2lab.load_state_dict(torch.load(PROXY2LAB_PATH, map_location=device))\n",
    "else:\n",
    "    print(\"No pre-trained model found. Starting from scratch.\")\n",
    "    optimizer = optim.Adam(proxy2lab.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracies_val = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        proxy2lab.train()\n",
    "        for i, (x, y) in enumerate(tqdm(mnist_train)):\n",
    "            x = x.to(device)\n",
    "            y = y.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = proxy2lab(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            losses_train.append(loss.item())\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            proxy2lab.eval()\n",
    "            total_val_loss = 0\n",
    "            total_val_accu = 0\n",
    "\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                y = y.float().to(device)\n",
    "                y_pred = proxy2lab(x)\n",
    "                val_loss = loss_fn(y_pred, y)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                lab_pred = y_pred.argmax(axis=1)\n",
    "                lab_true = y.argmax(axis=1)\n",
    "                acc = (lab_pred == lab_true).float().mean().item()\n",
    "                total_val_accu += acc\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(mnist_val)\n",
    "            avg_val_accu = total_val_accu / len(mnist_val)\n",
    "            accuracies_val.append(avg_val_accu)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}, Accuracy: {avg_val_accu:.4f}\")\n",
    "            losses_val.append(avg_val_loss)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    torch.save(proxy2lab.state_dict(), PROXY2LAB_PATH)\n",
    "\n",
    "    nb_minibatches = len(mnist_train)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    axes[0].plot(losses_train, label='Training Loss', color='blue')\n",
    "    axes[0].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                losses_val, label='Validation Loss', color='orange')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('VAE Proxy to Group Cross Entropy')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_xticks([])\n",
    "    xlim = axes[0].get_xlim()\n",
    "\n",
    "    axes[1].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                accuracies_val, label='Validation Accuracy', color='green')\n",
    "    axes[1].set_xlabel('Batch Iterations')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('VAE Proxy to Group Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].set_xlim(xlim)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11285c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy2lab.eval()\n",
    "\n",
    "lab_true_all = []\n",
    "lab_pred_all = []\n",
    "\n",
    "for x, y in mnist_test:\n",
    "    x = x.to(device)\n",
    "    y = y.float().to(device)\n",
    "    lab_true = y.argmax(axis=1)\n",
    "    lab_true_all.append(lab_true)\n",
    "\n",
    "    y_pred = proxy2lab(x)\n",
    "    lab_pred = y_pred.argmax(axis=1)\n",
    "    lab_pred_all.append(lab_pred)\n",
    "lab_true_all = torch.cat(lab_true_all).detach().cpu()\n",
    "lab_pred_all = torch.cat(lab_pred_all).detach().cpu()\n",
    "confusion_matrix = torch.zeros(10, 10, dtype=torch.int64)\n",
    "for true, pred in zip(lab_true_all, lab_pred_all):\n",
    "    confusion_matrix[true, pred] += 1\n",
    "accuracy = (lab_true_all == lab_pred_all).float().mean().item()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(confusion_matrix, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title(f\"Confusion Matrix of AE Proxy to Label (acc.: {accuracy:.3f})\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(ticks=range(10), labels=range(10))\n",
    "plt.yticks(ticks=range(10), labels=range(10))\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2_DIM = 4  # Dimension of the latent space for invariant VAE\n",
    "INAE_PATH = \"../checkpoints/mnist/group_similar/invariant_ae.pth\"\n",
    "# make cvae not trainable\n",
    "for param in cvae.parameters():\n",
    "    param.requires_grad = False\n",
    "# make invariant vae trainable\n",
    "inae = InvariantAutoEncoder(28 * 28, 128, Z2_DIM, cvae=cvae)\n",
    "inae = inae.to(device)\n",
    "\n",
    "if os.path.exists(INAE_PATH):\n",
    "    print(\"Loading pre-trained Invariant AE model...\")\n",
    "    inae.load_state_dict(torch.load(INAE_PATH, map_location=device))\n",
    "else:\n",
    "    print(\"No pre-trained model found. Starting from scratch.\")\n",
    "\n",
    "    optimizer = optim.Adam(inae.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        inae.train()\n",
    "        for i, (x, y) in enumerate(tqdm(mnist_train)):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_x = inae(x)\n",
    "            loss = loss_fn(recon_x, x)\n",
    "            loss.backward()\n",
    "            losses_train.append(loss.item())\n",
    "            optimizer.step()\n",
    "            # print(f\"{i + 1:>3}/{len(mnist_train)}: {loss.item():.4f}\", end=\"\\r\")\n",
    "        with torch.no_grad():\n",
    "            inae.eval()\n",
    "            total_val_loss = 0\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                recon_x = inae(x)\n",
    "                val_loss = loss_fn(recon_x, x)\n",
    "                total_val_loss += val_loss.item()\n",
    "            avg_val_loss = total_val_loss / len(mnist_val)\n",
    "            losses_val.append(avg_val_loss)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "    torch.save(inae.state_dict(), INAE_PATH)\n",
    "\n",
    "    nb_minibatches = len(mnist_train)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses_train, label='Training Loss', color='blue')\n",
    "    plt.plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "             losses_val, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Batch Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_SAMPLES = 8\n",
    "NB_subplots = 8\n",
    "nb_figures = NB_SAMPLES // NB_subplots\n",
    "\n",
    "mnist_train, mnist_val, mnist_test = get_mnist_dataloaders(\"../data\", one_hot=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "for x, y_real in mnist_test:\n",
    "    # print(x.shape, y.shape)\n",
    "    x = x.to(device)\n",
    "    y = get_merged_labels(y_real, merge_group)\n",
    "    y = y.to(device)\n",
    "    break\n",
    "\n",
    "x_recon_cvae = cvae(x, y)[0]\n",
    "y_shuffled = y[torch.randperm(y.size(0))]\n",
    "x_shuffled_recon, mu, logvar = cvae(x, y_shuffled)\n",
    "x_recon_inae = inae(x)\n",
    "\n",
    "for f in range(nb_figures):\n",
    "    fig, axs = plt.subplots(4, NB_subplots, figsize=(NB_subplots*2, 8), tight_layout=True)\n",
    "    for idx in range(min(NB_subplots, NB_SAMPLES - f * NB_subplots)):\n",
    "        axs[0, idx].imshow(x[f * NB_subplots + idx].cpu().reshape(28, 28), cmap=\"gray\")\n",
    "        axs[0, idx].set_title(f\"{y_real[idx].item()} in {merge_group[y[idx].argmax()]}\")\n",
    "        axs[0, idx].set_xticks([])\n",
    "        axs[0, idx].set_yticks([])\n",
    "\n",
    "        axs[1, idx].imshow(x_recon_cvae[f * NB_subplots + idx].detach().cpu().reshape(28, 28), cmap=\"gray\")\n",
    "        axs[1, idx].set_title(\" \")\n",
    "        axs[1, idx].set_xticks([])\n",
    "        axs[1, idx].set_yticks([])\n",
    "\n",
    "        axs[2, idx].imshow(x_shuffled_recon[idx].detach().cpu().reshape(28, 28), cmap=\"gray\")\n",
    "        axs[2, idx].set_title(f\"$S$ = {merge_group[y_shuffled[idx].argmax()]}\")\n",
    "        axs[2, idx].set_xticks([])\n",
    "        axs[2, idx].set_yticks([])\n",
    "\n",
    "        axs[3, idx].imshow(x_recon_inae[f * NB_subplots + idx].detach().cpu().reshape(28, 28), cmap=\"gray\")\n",
    "        axs[3, idx].set_title(\" \")\n",
    "        axs[3, idx].set_xticks([])\n",
    "        axs[3, idx].set_yticks([])\n",
    "    axs[0, 0].set_ylabel(\"Original\", fontsize=15)\n",
    "    axs[1, 0].set_ylabel(\"CVAE Recon.\", fontsize=15)\n",
    "    axs[2, 0].set_ylabel(\"Shuffled Recon.\", fontsize=15)\n",
    "    axs[3, 0].set_ylabel(\"Inv AE Recon.\", fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e233622",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 3\n",
    "x = []\n",
    "y = []\n",
    "for x_tmp, y_tmp in mnist_test:\n",
    "    x.append(x_tmp)\n",
    "    y.append(y_tmp)\n",
    "x = torch.cat(x).to(device)\n",
    "y = torch.cat(y).to(device)\n",
    "label = y.detach().cpu().flatten()\n",
    "x_recon_inv_vae = inae(x)\n",
    "z = inae.encoder(x)\n",
    "z = z.detach().cpu()\n",
    "z_centers = []\n",
    "for i in range(10):\n",
    "    center = z[label == i].mean(axis=0)\n",
    "    z_centers.append(center)\n",
    "z_centers = torch.stack(z_centers)\n",
    "\n",
    "fig, axes = plt.subplots(Z2_DIM - 1, Z2_DIM - 1, figsize=(5 * (Z2_DIM - 1), 5 * (Z2_DIM - 1)), tight_layout=True)\n",
    "for i in range(Z2_DIM - 1):\n",
    "    for j in range(1, Z2_DIM):\n",
    "        if i < j:\n",
    "            im = axes[i, j-1].scatter(z_centers[:, i], z_centers[:, j],\n",
    "                                    c=range(10), cmap=\"tab10\", vmin=-.5, vmax=9.5)\n",
    "            axes[i, j-1].scatter(z[:, i], z[:, j],\n",
    "                            c=label, cmap=\"tab10\", alpha=0.1)\n",
    "            # axes[i, j-1].set_aspect('equal', adjustable='box')\n",
    "            # axes[i, j-1].set_xlim(-LIMIT, LIMIT)\n",
    "            # axes[i, j-1].set_ylim(-LIMIT, LIMIT)\n",
    "            for d in range(10):\n",
    "                axes[i, j-1].text(z_centers[d][i], z_centers[d][j], str(d), fontsize=15, ha='center', va='center')\n",
    "        else:\n",
    "            axes[i, j-1].axis('off')\n",
    "\n",
    "for i in range(Z2_DIM - 1):\n",
    "    for j in range(Z2_DIM - 1):\n",
    "        if i == j :\n",
    "            axes[i, j].set_ylabel(f\"$z_2$[{i}]\", fontsize=15)\n",
    "            axes[i, j].set_xlabel(f\"$z_2$[{j + 1}]\", fontsize=15)\n",
    "        elif i > j:\n",
    "            axes[i, j].axis('off')\n",
    "        # else:\n",
    "        #     axes[i, j].set_xticks([])\n",
    "        #     axes[i, j].set_yticks([])\n",
    "\n",
    "plt.suptitle(\"Latent Space of Invariant AE\", fontsize=20)\n",
    "cbar = plt.colorbar(im, ax=axes, orientation='horizontal',\n",
    "                    fraction=0.02, pad=0.0)\n",
    "cbar.set_label('Digit Label', fontsize=15)\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.set_ticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY2INVAR_PATH = \"../checkpoints/mnist/group_similar/aeproxy2invar.pth\"\n",
    "\n",
    "for param in cvae.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in inae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "proxy2invar = ProxyRep2InvarRep(autoencoder=inae, reparameterize=False)\n",
    "proxy2invar = proxy2invar.to(device)\n",
    "\n",
    "if os.path.exists(PROXY2INVAR_PATH):\n",
    "    print(\"Loading pre-trained Proxy2Invar model...\")\n",
    "    proxy2invar.load_state_dict(torch.load(PROXY2INVAR_PATH, map_location=device))\n",
    "else:\n",
    "    print(\"No pre-trained model found. Starting from scratch.\")\n",
    "    optimizer = optim.Adam(proxy2invar.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        proxy2invar.train()\n",
    "        for i, (x, y) in enumerate(tqdm(mnist_train)):\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # Get real invariant representation from CVAE\n",
    "            with torch.no_grad():\n",
    "                z_invar = inae.cvae.encoder(x)\n",
    "                mu = z_invar[:, :inae.cvae.latent_dim]\n",
    "                logvar = z_invar[:, inae.cvae.latent_dim:]\n",
    "                z_invar = inae.cvae.reparameterize(mu, logvar)\n",
    "            \n",
    "            z_invar_pred = proxy2invar(x)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(z_invar_pred, z_invar)\n",
    "            loss.backward()\n",
    "            losses_train.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "        proxy2invar.eval()\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                z_invar = inae.cvae.encoder(x)\n",
    "                mu = z_invar[:, :inae.cvae.latent_dim]\n",
    "                logvar = z_invar[:, inae.cvae.latent_dim:]\n",
    "                z_invar = inae.cvae.reparameterize(mu, logvar)\n",
    "\n",
    "                z_invar_pred = proxy2invar(x)\n",
    "                val_loss = loss_fn(z_invar_pred, z_invar)\n",
    "                total_val_loss += val_loss.item()\n",
    "            avg_val_loss = total_val_loss / len(mnist_val)\n",
    "            losses_val.append(avg_val_loss)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    torch.save(proxy2invar.state_dict(), PROXY2INVAR_PATH)\n",
    "\n",
    "    nb_minibatches = len(mnist_train)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses_train, label='Training Loss', color='blue')\n",
    "    plt.plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                losses_val, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Batch Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('AE Proxy to Invariant Representation MSE')\n",
    "    plt.legend()\n",
    "    plt.xscale('log')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4312e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY2GROUP_PATH = \"../checkpoints/mnist/group_similar/aeproxy2group.pth\"\n",
    "proxy2group = ProxyRep2Label(autoencoder=inae, reparameterize=False, nb_labels=len(merge_group))\n",
    "proxy2group = proxy2group.to(device)\n",
    "\n",
    "mnist_train, mnist_val, mnist_test = get_mnist_dataloaders(\"../data\", one_hot=False, batch_size=BATCH_SIZE)\n",
    "if os.path.exists(PROXY2GROUP_PATH):\n",
    "    print(\"Loading pre-trained Proxy2Group model...\")\n",
    "    proxy2group.load_state_dict(torch.load(PROXY2GROUP_PATH, map_location=device))\n",
    "else:\n",
    "    print(\"No pre-trained model found. Starting from scratch.\")\n",
    "    optimizer = optim.Adam(proxy2group.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracies_val = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        proxy2group.train()\n",
    "        for i, (x, y) in enumerate(tqdm(mnist_train)):\n",
    "            x = x.to(device)\n",
    "            y = get_merged_labels(y, merge_group).float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = proxy2group(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            losses_train.append(loss.item())\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            proxy2group.eval()\n",
    "            total_val_loss = 0\n",
    "            total_val_accu = 0\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                y = get_merged_labels(y, merge_group).float().to(device)\n",
    "                y_pred = proxy2group(x)\n",
    "                val_loss = loss_fn(y_pred, y)\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "                group_pred = y_pred.argmax(dim=1)\n",
    "                group_true = y.argmax(dim=1)\n",
    "                acc = (group_pred == group_true).float().mean().item()\n",
    "                total_val_accu += acc\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(mnist_val)\n",
    "            avg_val_accu = total_val_accu / len(mnist_val)\n",
    "            accuracies_val.append(avg_val_accu)\n",
    "            losses_val.append(avg_val_loss)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}, Accuracy: {avg_val_accu:.4f}\")\n",
    "    torch.save(proxy2group.state_dict(), PROXY2GROUP_PATH)\n",
    "\n",
    "    nb_minibatches = len(mnist_train)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    axes[0].plot(losses_train, label='Training Loss', color='blue')\n",
    "    axes[0].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                losses_val, label='Validation Loss', color='orange')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('VAE Proxy to Label Cross Entropy')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_xticks([])\n",
    "    xlim = axes[0].get_xlim()\n",
    "\n",
    "    axes[1].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                accuracies_val, label='Validation Accuracy', color='green')\n",
    "    axes[1].set_xlabel('Batch Iterations')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('VAE Proxy to Label Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].set_xlim(xlim)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6908010",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy2group.eval()\n",
    "group_true_all = []\n",
    "group_pred_all = []\n",
    "with torch.no_grad():\n",
    "    for x, y in mnist_test:\n",
    "        x = x.to(device)\n",
    "        y = get_merged_labels(y, merge_group).float().to(device)\n",
    "        y_pred = proxy2group(x)\n",
    "        group_pred = y_pred.argmax(dim=1)\n",
    "        group_true = y.argmax(dim=1)\n",
    "\n",
    "        group_true_all.append(group_true.cpu())\n",
    "        group_pred_all.append(group_pred.cpu())\n",
    "group_true_all = torch.cat(group_true_all)\n",
    "group_pred_all = torch.cat(group_pred_all)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(group_true_all, group_pred_all)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.xlabel('Predicted Group')\n",
    "plt.ylabel('True Group')\n",
    "plt.title('Confusion Matrix for Group Predictions')\n",
    "plt.xticks(range(len(merge_group)), [str(g) for g in merge_group], rotation=45)\n",
    "plt.yticks(range(len(merge_group)), [str(g) for g in merge_group])\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY2LAB_PATH = \"../checkpoints/mnist/group_similar/aeproxy2lab.pth\"\n",
    "\n",
    "proxy2lab = ProxyRep2Label(autoencoder=inae, reparameterize=False, nb_labels=10)\n",
    "proxy2lab = proxy2lab.to(device)\n",
    "mnist_train, mnist_val, mnist_test = get_mnist_dataloaders(\"../data\", one_hot=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "if os.path.exists(PROXY2LAB_PATH):\n",
    "    print(\"Loading pre-trained Proxy2Label model...\")\n",
    "    proxy2lab.load_state_dict(torch.load(PROXY2LAB_PATH, map_location=device))\n",
    "else:\n",
    "    print(\"No pre-trained model found. Starting from scratch.\")\n",
    "    optimizer = optim.Adam(proxy2lab.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracies_val = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        proxy2lab.train()\n",
    "        for i, (x, y) in enumerate(tqdm(mnist_train)):\n",
    "            x = x.to(device)\n",
    "            y = y.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = proxy2lab(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            losses_train.append(loss.item())\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            proxy2lab.eval()\n",
    "            total_val_loss = 0\n",
    "            total_val_accu = 0\n",
    "\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                y = y.float().to(device)\n",
    "                y_pred = proxy2lab(x)\n",
    "                val_loss = loss_fn(y_pred, y)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                lab_pred = y_pred.argmax(axis=1)\n",
    "                lab_true = y.argmax(axis=1)\n",
    "                acc = (lab_pred == lab_true).float().mean().item()\n",
    "                total_val_accu += acc\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(mnist_val)\n",
    "            avg_val_accu = total_val_accu / len(mnist_val)\n",
    "            accuracies_val.append(avg_val_accu)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}, Accuracy: {avg_val_accu:.4f}\")\n",
    "            losses_val.append(avg_val_loss)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    torch.save(proxy2lab.state_dict(), PROXY2LAB_PATH)\n",
    "\n",
    "    nb_minibatches = len(mnist_train)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    axes[0].plot(losses_train, label='Training Loss', color='blue')\n",
    "    axes[0].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                losses_val, label='Validation Loss', color='orange')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('AE Proxy to Label Cross Entropy')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_xticks([])\n",
    "    xlim = axes[0].get_xlim()\n",
    "\n",
    "    axes[1].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                accuracies_val, label='Validation Accuracy', color='green')\n",
    "    axes[1].set_xlabel('Batch Iterations')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('AE Proxy to Label Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].set_xlim(xlim)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0862b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY2LAB_PATH = \"../checkpoints/mnist/group_similar/aeproxy2lab.pth\"\n",
    "\n",
    "proxy2lab = ProxyRep2Label(autoencoder=inae, reparameterize=False, nb_labels=10)\n",
    "proxy2lab = proxy2lab.to(device)\n",
    "\n",
    "if os.path.exists(PROXY2LAB_PATH):\n",
    "    print(\"Loading pre-trained Proxy2Label model...\")\n",
    "    proxy2lab.load_state_dict(torch.load(PROXY2LAB_PATH, map_location=device))\n",
    "else:\n",
    "    print(\"No pre-trained model found. Starting from scratch.\")\n",
    "    optimizer = optim.Adam(proxy2lab.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracies_val = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        proxy2lab.train()\n",
    "        for i, (x, y) in enumerate(tqdm(mnist_train)):\n",
    "            x = x.to(device)\n",
    "            y = y.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = proxy2lab(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            losses_train.append(loss.item())\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            proxy2lab.eval()\n",
    "            total_val_loss = 0\n",
    "            total_val_accu = 0\n",
    "\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                y = y.float().to(device)\n",
    "                y_pred = proxy2lab(x)\n",
    "                val_loss = loss_fn(y_pred, y)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                lab_pred = y_pred.argmax(axis=1)\n",
    "                lab_true = y.argmax(axis=1)\n",
    "                acc = (lab_pred == lab_true).float().mean().item()\n",
    "                total_val_accu += acc\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(mnist_val)\n",
    "            avg_val_accu = total_val_accu / len(mnist_val)\n",
    "            accuracies_val.append(avg_val_accu)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}, Accuracy: {avg_val_accu:.4f}\")\n",
    "            losses_val.append(avg_val_loss)\n",
    "            print(f\"\\nEpoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    torch.save(proxy2lab.state_dict(), PROXY2LAB_PATH)\n",
    "\n",
    "    nb_minibatches = len(mnist_train)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    axes[0].plot(losses_train, label='Training Loss', color='blue')\n",
    "    axes[0].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                losses_val, label='Validation Loss', color='orange')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('AE Proxy to Label Cross Entropy')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_xticks([])\n",
    "    xlim = axes[0].get_xlim()\n",
    "\n",
    "    axes[1].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                accuracies_val, label='Validation Accuracy', color='green')\n",
    "    axes[1].set_xlabel('Batch Iterations')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('AE Proxy to Label Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].set_xlim(xlim)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy2lab.eval()\n",
    "\n",
    "lab_true_all = []\n",
    "lab_pred_all = []\n",
    "\n",
    "for x, y in mnist_test:\n",
    "    x = x.to(device)\n",
    "    y = y.float().to(device)\n",
    "    lab_true = y.argmax(axis=1)\n",
    "    lab_true_all.append(lab_true)\n",
    "\n",
    "    y_pred = proxy2lab(x)\n",
    "    lab_pred = y_pred.argmax(axis=1)\n",
    "    lab_pred_all.append(lab_pred)\n",
    "lab_true_all = torch.cat(lab_true_all).detach().cpu()\n",
    "lab_pred_all = torch.cat(lab_pred_all).detach().cpu()\n",
    "confusion_matrix = torch.zeros(10, 10, dtype=torch.int64)\n",
    "for true, pred in zip(lab_true_all, lab_pred_all):\n",
    "    confusion_matrix[true, pred] += 1\n",
    "accuracy = (lab_true_all == lab_pred_all).float().mean().item()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(confusion_matrix, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title(f\"Confusion Matrix of AE Proxy to Label (acc.: {accuracy:.3f})\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(ticks=range(10), labels=range(10))\n",
    "plt.yticks(ticks=range(10), labels=range(10))\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "invrep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
