{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set the GPUs to use\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from src.mnist.datasets import get_mnist_dataloaders, convert_flattened_to_image, get_merged_labels\n",
    "from src.mnist.models import InfoBottleneckClassifier, ProxyRep2Label\n",
    "from src.mnist.losses import InfoBottleneck_Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "# BETA = 5e-4  # Scaled by batch size for stability\n",
    "BETA = 0.001  # Scaled by batch size for stability\n",
    "REDUCTION = 'mean'\n",
    "EPOCHS = 50\n",
    "LR = 5e-4  # Learning rate\n",
    "IBGROUP_PATH = \"../checkpoints/mnist/group_similar/ib_groupifier.pth\"\n",
    "\n",
    "mnist_train, mnist_val, mnist_test = get_mnist_dataloaders(\"../data\", one_hot=False)\n",
    "\n",
    "merge_group = [\n",
    "    [0, 6],\n",
    "    [1],\n",
    "    [4, 7, 9],\n",
    "    [2, 3, 5, 8]\n",
    "]\n",
    "\n",
    "ib_groupifier = InfoBottleneckClassifier(\n",
    "    input_dim=28 * 28,\n",
    "    encoder_layer_sizes=[128, 128],\n",
    "    latent_dim=64,\n",
    "    mlp_layer_sizes=[256, 512, 256],\n",
    "    nb_labels=len(merge_group)\n",
    ").to(device)\n",
    "\n",
    "if os.path.exists(IBGROUP_PATH):\n",
    "    print(f\"Loading InfoBottleneck model from {IBGROUP_PATH}\")\n",
    "    ib_groupifier.load_state_dict(torch.load(IBGROUP_PATH))\n",
    "else:\n",
    "    print(f\"InfoBottleneck model not found at {IBGROUP_PATH},\\nstarting training from scratch.\")\n",
    "    optimizer = optim.Adam(ib_groupifier.parameters(), lr=LR)\n",
    "    loss_fn = InfoBottleneck_Loss(beta=BETA, reduction=REDUCTION)\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracies_val = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        ib_groupifier.train()\n",
    "\n",
    "        for i, (x, y) in enumerate(mnist_train):\n",
    "            x = x.to(device)\n",
    "            y = get_merged_labels(y, merge_group)\n",
    "            y = y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, mu, logvar = ib_groupifier(x)\n",
    "            loss = loss_fn(y, y_pred, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses_train.append(loss.item())\n",
    "            print(f\"Epoch [{epoch + 1}/{EPOCHS}], Step [{i + 1}/{len(mnist_train)}], Loss: {loss.item():.4f}\", end='\\r')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ib_groupifier.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                y = get_merged_labels(y, merge_group)\n",
    "                y = y.float().to(device)\n",
    "\n",
    "                y_pred, mu, logvar = ib_groupifier(x)\n",
    "                loss = loss_fn(y, y_pred, mu, logvar)\n",
    "                val_loss += loss.item()\n",
    "                group_pred = torch.argmax(y_pred, dim=-1)\n",
    "                y = torch.argmax(y, dim=-1)\n",
    "                correct += (group_pred == y).sum().item()\n",
    "                total += y.size(0)\n",
    "            val_loss /= len(mnist_val)\n",
    "            accuracy = correct / total\n",
    "            losses_val.append(val_loss)\n",
    "            accuracies_val.append(accuracy)\n",
    "            print(f\"\\nEpoch [{epoch + 1}/{EPOCHS}], Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    torch.save(ib_groupifier.state_dict(), IBGROUP_PATH)\n",
    "\n",
    "    nb_minibatches = len(mnist_train)\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    axes[0].plot(losses_train, label='Training Loss', color='blue')\n",
    "    axes[0].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                losses_val, label='Validation Loss', color='orange')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('VAE Proxy to Group Cross Entropy')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_xticks([])\n",
    "    xlim = axes[0].get_xlim()\n",
    "    axes[1].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                accuracies_val, label='Validation Accuracy', color='green')\n",
    "    axes[1].set_xlabel('Batch Iterations')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('VAE Proxy to Group Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].set_xlim(xlim)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802eed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_minibatches = len(mnist_train)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "axes[0].plot(losses_train, label='Training Loss', color='blue')\n",
    "axes[0].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "            losses_val, label='Validation Loss', color='orange')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('VAE Proxy to Group Cross Entropy')\n",
    "axes[0].legend()\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xticks([])\n",
    "xlim = axes[0].get_xlim()\n",
    "axes[1].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "            accuracies_val, label='Validation Accuracy', color='green')\n",
    "axes[1].set_xlabel('Batch Iterations')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('VAE Proxy to Group Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlim(xlim)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169ceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ib_groupifier.eval()\n",
    "group_true_all = []\n",
    "group_pred_all = []\n",
    "with torch.no_grad():\n",
    "    for x, y in mnist_test:\n",
    "        x = x.to(device)\n",
    "        y = get_merged_labels(y, merge_group).float().to(device)\n",
    "        y_pred = ib_groupifier(x)[0]\n",
    "        group_pred = y_pred.argmax(dim=-1)\n",
    "        group_true = y.argmax(dim=-1)\n",
    "        group_true_all.append(group_true.cpu().numpy())\n",
    "        group_pred_all.append(group_pred.cpu().numpy())\n",
    "group_true_all = np.concatenate(group_true_all)\n",
    "group_pred_all = np.concatenate(group_pred_all)\n",
    "\n",
    "cm = confusion_matrix(group_true_all, group_pred_all)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.xlabel('Predicted Group')\n",
    "plt.ylabel('True Group')\n",
    "plt.title('Confusion Matrix for Group Predictions')\n",
    "plt.xticks(range(len(merge_group)), [str(g) for g in merge_group], rotation=45)\n",
    "plt.yticks(range(len(merge_group)), [str(g) for g in merge_group])\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "IBCLASS_PATH = \"../checkpoints/mnist/group_similar/ib_classifier.pth\"\n",
    "\n",
    "mnist_train, mnist_val, mnist_test = get_mnist_dataloaders(\"../data\", one_hot=True)\n",
    "\n",
    "z2lab = ProxyRep2Label(autoencoder=ib_groupifier, reparameterize=True, nb_labels=10)\n",
    "z2lab = z2lab.to(device)\n",
    "\n",
    "if os.path.exists(IBCLASS_PATH):\n",
    "    print(f\"Loading ProxyRep2Label model from {IBCLASS_PATH}\")\n",
    "    z2lab.load_state_dict(torch.load(IBCLASS_PATH))\n",
    "else:\n",
    "    print(f\"ProxyRep2Label model not found at {IBCLASS_PATH},\\nstarting training from scratch.\")\n",
    "    optimizer = optim.Adam(z2lab.parameters(), lr=LR)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracies_val = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        z2lab.train()\n",
    "\n",
    "        for i, (x, y) in enumerate(mnist_train):\n",
    "            x = x.to(device)\n",
    "            y = y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = z2lab(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses_train.append(loss.item())\n",
    "            print(f\"Epoch [{epoch + 1}/{EPOCHS}], Step [{i + 1}/{len(mnist_train)}], Loss: {loss.item():.4f}\", end='\\r')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z2lab.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for x, y in mnist_val:\n",
    "                x = x.to(device)\n",
    "                y = y.float().to(device)\n",
    "\n",
    "                y_pred = z2lab(x)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "                val_loss += loss.item()\n",
    "                y_pred = y_pred.argmax(dim=-1)\n",
    "                y = y.argmax(dim=-1)\n",
    "                correct += (y_pred == y).sum().item()\n",
    "                total += y.size(0)\n",
    "            val_loss /= len(mnist_val)\n",
    "            accuracy = correct / total\n",
    "            losses_val.append(val_loss)\n",
    "            accuracies_val.append(accuracy)\n",
    "            print(f\"\\nEpoch [{epoch + 1}/{EPOCHS}], Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    torch.save(z2lab.state_dict(), IBCLASS_PATH)\n",
    "\n",
    "    minibatches = len(mnist_train)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    axes[0].plot(losses_train, label='Training Loss', color='blue')\n",
    "    axes[0].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                losses_val, label='Validation Loss', color='orange')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('VAE Proxy to Group Cross Entropy')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_xticks([])\n",
    "    xlim = axes[0].get_xlim()\n",
    "    axes[1].plot(range(nb_minibatches, nb_minibatches * EPOCHS + 1, nb_minibatches),\n",
    "                accuracies_val, label='Validation Accuracy', color='green')\n",
    "    axes[1].set_xlabel('Batch Iterations')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('VAE Proxy to Group Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].set_xlim(xlim)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2lab.eval()\n",
    "\n",
    "lab_true_all = []\n",
    "lab_pred_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in mnist_test:\n",
    "        x = x.to(device)\n",
    "        y = y.float().to(device)\n",
    "        y_pred = z2lab(x)\n",
    "        lab_pred = y_pred.argmax(dim=-1)\n",
    "        lab_true = y.argmax(dim=-1)\n",
    "        lab_true_all.append(lab_true.cpu().numpy())\n",
    "        lab_pred_all.append(lab_pred.cpu().numpy())\n",
    "lab_true_all = np.concatenate(lab_true_all)\n",
    "lab_pred_all = np.concatenate(lab_pred_all)\n",
    "accuracy = (lab_true_all == lab_pred_all).mean()\n",
    "cm = confusion_matrix(lab_true_all, lab_pred_all)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title(f\"Confusion Matrix for Digit Predictions (acc.: {accuracy:.3f})\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(ticks=range(10), labels=range(10))\n",
    "plt.yticks(ticks=range(10), labels=range(10))\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14449789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "invrep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
